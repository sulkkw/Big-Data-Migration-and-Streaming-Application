{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: KO KO WIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os \n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0'\n",
    "import sys \n",
    "import json \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType \n",
    "import pygeohash as gh\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = [\"Producer01\", \"Producer02\", \"Producer03\"] \n",
    "topics = 'Producer01,Producer02,Producer03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =  (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName('Streaming Application')\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_stream_df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option('kafka.bootstrap.servers', 'localhost:9092')\n",
    "    .option('subscribe', topics) \n",
    "    .load()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: binary]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_stream_df.select('value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema to be use in json parsing\n",
    "doc_schema = (\n",
    "    StructType()\n",
    "    .add(\"latitude\", 'string')\n",
    "    .add(\"longitude\", 'string')\n",
    "    .add(\"air_temperature_celcius\", 'string')\n",
    "    .add(\"relative_humidity\", 'string')\n",
    "    .add(\"windspeed_knots\", 'string')\n",
    "    .add(\"max_wind_speed\", 'string')\n",
    "    .add(\"precipitation\", 'string')\n",
    "    .add(\"GHI_w/m2\", 'string')\n",
    "    .add(\"precipitation_flag\", 'string')\n",
    "    .add(\"precipitation_value\", 'string')\n",
    "    .add(\"time\", 'string')\n",
    "    .add(\"created_date\", 'string')\n",
    "    .add(\"producer_information\", 'string')\n",
    "    .add(\"confidence\", 'string')\n",
    "    .add(\"surface_temperature_celcius\", 'string')\n",
    "    .add(\"created_time\", 'string')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- from_json(value): struct (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |    |-- air_temperature_celcius: string (nullable = true)\n",
      " |    |-- relative_humidity: string (nullable = true)\n",
      " |    |-- windspeed_knots: string (nullable = true)\n",
      " |    |-- max_wind_speed: string (nullable = true)\n",
      " |    |-- precipitation: string (nullable = true)\n",
      " |    |-- GHI_w/m2: string (nullable = true)\n",
      " |    |-- precipitation_flag: string (nullable = true)\n",
      " |    |-- precipitation_value: string (nullable = true)\n",
      " |    |-- time: string (nullable = true)\n",
      " |    |-- created_date: string (nullable = true)\n",
      " |    |-- producer_information: string (nullable = true)\n",
      " |    |-- confidence: string (nullable = true)\n",
      " |    |-- surface_temperature_celcius: string (nullable = true)\n",
      " |    |-- created_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Parse value in JSON string\n",
    "(\n",
    "    topic_stream_df\n",
    "    .select( col('value').cast(\"string\"))\n",
    "    .select(from_json('value', doc_schema))\n",
    "    .printSchema()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch_df, batch_id): \n",
    "    \n",
    "    client = MongoClient() \n",
    "    db = client.fit3182_assignment_db \n",
    "    collections = db.stream \n",
    "\n",
    "    climate = {} \n",
    "    aqua_arr = []\n",
    "    terra_arr = []\n",
    "    hotspot = []\n",
    "    fire_event = []\n",
    "    fire_dict = {}\n",
    "    \n",
    "    raw_data = batch_df.collect()\n",
    "    \n",
    "    for record in raw_data.asDict(): \n",
    "        prod_id = record[\"producer_information\"]\n",
    "\n",
    "        if prod_id == \"Event Producer 1 (Climate)\": \n",
    "            climate = record\n",
    "\n",
    "        elif prod_id == \"Event Producer 2 (Aqua)\": \n",
    "            aqua_arr.append(record) \n",
    "\n",
    "        elif prod_id == \"Event Producer 3 (Terra)\": \n",
    "            terra_arr.append(record) \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "    if climate != 0: \n",
    "        #get longitude&latitude of climate \n",
    "        climate_long = climate[\"longitude\"]\n",
    "        climate_lat = climate[\"lat\"] \n",
    "        \n",
    "        #look throguh aqua data, if geohash match add to hotspot array\n",
    "        for x in aqua_arr: \n",
    "            #get longitude&latitude of aqua satellite\n",
    "            aqua_long = x[\"longitude\"] \n",
    "            aqua_lat = x[\"latitude\"] \n",
    "            geo_hash_aqua = gh.encode(aqua_long, aqua_lat, precision=3)\n",
    "            geo_hash_cli = gh.encode(climate_long, climate_lat, precision=3)\n",
    "            \n",
    "            if geo_hash_aqua == geo_hash_cli: \n",
    "                hotspot.append(x) \n",
    "                #do something to date \n",
    "                x[\"date\"] = climate[\"created_date\"]\n",
    "\n",
    "        #look through terra data, if geohash match add to hotspot array\n",
    "        for e in terra_arr :\n",
    "            #get long and lat of terra satellite \n",
    "            terra_long = e[\"longitude\"] \n",
    "            terra_lat = e[\"latitude\"] \n",
    "            geo_hash_terra = gh.encode(terra_long, terra_lat, precision=3)\n",
    "            geo_hash_cli = gh.encode(climate_long, climate_lat, precision=3)\n",
    "            \n",
    "            if geo_hash_terra == geo_hash_cli: \n",
    "                hotspot.append(e) \n",
    "                e[\"date\"] = climate[\"created_date\"] \n",
    "        \n",
    "        #if hotspot data is empty add climate data to MongoDB\n",
    "        if len(hotspot) == 0: \n",
    "            try: \n",
    "                collections.insert(climate)\n",
    "            \n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "                \n",
    "        #if the hotspot data is not empty \n",
    "        else: \n",
    "            climate[\"hotspots\"] = hotspot\n",
    "            if len(aqua_arr) != 0 and len(terra_arr) != 0:\n",
    "                \n",
    "                for aq in aqua_arr: \n",
    "                    aqua_long = aq[\"longitude\"]\n",
    "                    aqua_lat = aq[\"latitude\"]\n",
    "\n",
    "                    for tr in terra_arr: \n",
    "                        terra_long = tr[\"longitude\"]\n",
    "                        terra_lat = tr[\"latitude\"] \n",
    "\n",
    "                        geo_hash_aqua = gh.encode(aqua_long, aqua_lat, precision=5) \n",
    "                        geo_hash_terra = gh.encode(terra_long, terra_lat, precision=5)\n",
    "                        \n",
    "                        #check if the locations of 2 stellites are the same \n",
    "                        if geo_hash_aqua == geo_hash_terra: \n",
    "\n",
    "                            #calculate average surface_temp and confidence of 2 satellites \n",
    "                            fire_dict[\"avg_surface_temperature\"] = (aq[\"surface_temperature_celcius\"] + \n",
    "                                                                    tr[\"surface_temperature_celcius\"]) / 2 \n",
    "                            fire_dict[\"avg_confidence\"] = (aq[\"confidence\"] + tr[\"confidence\"]) / 2 \n",
    "\n",
    "                            #check the event of the fire \n",
    "                            if climate[\"air_temperature_celcius\"] > 20 and climate[\"GHI_w/m2\"] > 180: \n",
    "                                fire_dict[\"event\"] = \"natural\"\n",
    "\n",
    "                            else: \n",
    "                                fire_dict[\"event\"] = \"other\"\n",
    "\n",
    "                            fire_event.append(fire_dict)\n",
    "\n",
    "            #add the fire data into the climate document \n",
    "            climate[\"fire\"] = fire_event \n",
    "\n",
    "            try: \n",
    "                collections.insert(climate)\n",
    "\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "    \n",
    "    client.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_writer = (\n",
    "    topic_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .foreachBatch(process_batch)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f2e1c799190>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "console_logger = (\n",
    "    topic_stream_df\n",
    "    .writeStream\n",
    "    .outputMode('append')\n",
    "    .format('console')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.fit3182_assignment_db \n",
    "collections = db.stream \n",
    "\n",
    "for col in collections.find({}): \n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
